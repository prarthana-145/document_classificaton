{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fba37d5-394f-4ee0-9b7b-49de371c6706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DocumentDataset(Dataset):\n",
    "    def __init__(self, img_dir, labels_excel, transform=None):\n",
    "        \"\"\"\n",
    "        img_dir:   path to folder containing images (e.g. 'data/classification/train')\n",
    "        labels_excel: filename of the Excel sheet in that folder\n",
    "        transform: torchvision transforms to apply\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # 1) Read the Excel file into a DataFrame\n",
    "        df = pd.read_csv(os.path.join(img_dir, labels_excel))\n",
    "\n",
    "        # 2) Build a mapping from class name → index\n",
    "        self.class_names = [' cheque', ' payslip', ' receipt', ' tax']\n",
    "        self.class_to_idx = {c:i for i,c in enumerate(self.class_names)}\n",
    "\n",
    "        # 3) Build a list of (image_path, label_index)\n",
    "        self.samples = []\n",
    "        for _, row in df.iterrows():\n",
    "            fname = row['filename']\n",
    "            full_path = os.path.join(img_dir, fname)\n",
    "            if not os.path.isfile(full_path):\n",
    "                continue  # skip missing files\n",
    "            # find which column has a 1\n",
    "            for cls in self.class_names:\n",
    "                if row[cls] == 1:\n",
    "                    label = self.class_to_idx[cls]\n",
    "                    self.samples.append((full_path, label))\n",
    "                    break\n",
    "        print(f\"Loaded {len(self.samples)} samples from {img_dir}\")\n",
    "\n",
    "        print(\"First 5 entries:\", self.samples[:5])\n",
    "\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "18df85d1-e96c-437a-8c92-9acc8635d481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 302 samples from C:\\Users\\praag\\Desktop\\scrapiq\\doc scanner\\financial classification.v1i.multiclass\\train\n",
      "First 5 entries: [('C:\\\\Users\\\\praag\\\\Desktop\\\\scrapiq\\\\doc scanner\\\\financial classification.v1i.multiclass\\\\train\\\\51_jpg.rf.e0e7740d132b86db19ea3bfd18bb14d0.jpg', 0), ('C:\\\\Users\\\\praag\\\\Desktop\\\\scrapiq\\\\doc scanner\\\\financial classification.v1i.multiclass\\\\train\\\\receipt_00207_png_jpg.rf.e046c0fa8968ce4657787e6f1b758e13.jpg', 2), ('C:\\\\Users\\\\praag\\\\Desktop\\\\scrapiq\\\\doc scanner\\\\financial classification.v1i.multiclass\\\\train\\\\33_jpg.rf.dd361eb32c7c387b408de5f432c1ab88.jpg', 0), ('C:\\\\Users\\\\praag\\\\Desktop\\\\scrapiq\\\\doc scanner\\\\financial classification.v1i.multiclass\\\\train\\\\Screenshot-from-2024-01-11-15-59-43_png.rf.db19d7fc74d12d398be3ae9ee0b9f9e5.jpg', 3), ('C:\\\\Users\\\\praag\\\\Desktop\\\\scrapiq\\\\doc scanner\\\\financial classification.v1i.multiclass\\\\train\\\\75_jpg.rf.dfedf5eef0e1407b5814881e54de94b4.jpg', 0)]\n",
      "Loaded 53 samples from C:\\Users\\praag\\Desktop\\scrapiq\\doc scanner\\financial classification.v1i.multiclass\\valid\n",
      "First 5 entries: [('C:\\\\Users\\\\praag\\\\Desktop\\\\scrapiq\\\\doc scanner\\\\financial classification.v1i.multiclass\\\\valid\\\\Screenshot-from-2024-01-11-16-01-19_png.rf.116b09ee06dda173e1ea378458614cbe.jpg', 3), ('C:\\\\Users\\\\praag\\\\Desktop\\\\scrapiq\\\\doc scanner\\\\financial classification.v1i.multiclass\\\\valid\\\\Untitled_jpeg.rf.05cd111bd1833e18b1324b7c98d28efb.jpg', 2), ('C:\\\\Users\\\\praag\\\\Desktop\\\\scrapiq\\\\doc scanner\\\\financial classification.v1i.multiclass\\\\valid\\\\receipt_00277_png_jpg.rf.1bc3355fd0af0bc28b738a9bd2c1c86c.jpg', 2), ('C:\\\\Users\\\\praag\\\\Desktop\\\\scrapiq\\\\doc scanner\\\\financial classification.v1i.multiclass\\\\valid\\\\7sNUpKDJ_jpg.rf.1ae53673a84becabe999938cebcbcaf6.jpg', 1), ('C:\\\\Users\\\\praag\\\\Desktop\\\\scrapiq\\\\doc scanner\\\\financial classification.v1i.multiclass\\\\valid\\\\main-qimg-c5b91d9240b515d49f7a38c80042b998-pjlq_jpeg.rf.295452ac13c6084b74f14b086e0096ed.jpg', 0)]\n",
      "Loaded 23 samples from C:\\Users\\praag\\Desktop\\scrapiq\\doc scanner\\financial classification.v1i.multiclass\\test\n",
      "First 5 entries: [('C:\\\\Users\\\\praag\\\\Desktop\\\\scrapiq\\\\doc scanner\\\\financial classification.v1i.multiclass\\\\test\\\\93_jpg.rf.703f7b1d82ce98049a05707d2749f2c1.jpg', 0), ('C:\\\\Users\\\\praag\\\\Desktop\\\\scrapiq\\\\doc scanner\\\\financial classification.v1i.multiclass\\\\test\\\\25-1-_jpg.rf.0a02cac8e2c6f3f51d212a4a2899ded1.jpg', 3), ('C:\\\\Users\\\\praag\\\\Desktop\\\\scrapiq\\\\doc scanner\\\\financial classification.v1i.multiclass\\\\test\\\\Screenshot-from-2024-01-11-15-58-02_png.rf.cb480e1e1a61832e0648a6ca07c2e3ce.jpg', 3), ('C:\\\\Users\\\\praag\\\\Desktop\\\\scrapiq\\\\doc scanner\\\\financial classification.v1i.multiclass\\\\test\\\\receipt_00051_png_jpg.rf.bd56e4f14cad1b36bfd2341c8ddb448c.jpg', 2), ('C:\\\\Users\\\\praag\\\\Desktop\\\\scrapiq\\\\doc scanner\\\\financial classification.v1i.multiclass\\\\test\\\\receipt_00278_png_jpg.rf.2024bfcb23485f806b3fcc2a07412062.jpg', 2)]\n",
      "✅ Train class counts: Counter({3: 80, 0: 77, 1: 73, 2: 72})\n",
      "RAW image pixel range: 0 255\n",
      "TRANSFORMED tensor shape: torch.Size([3, 224, 224])\n",
      "TRANSFORMED pixel range: -181574516736.0 -372947232.0\n",
      "Transform pipeline: Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    ToTensor()\n",
      "    Lambda()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "Fixed TRANSFORMED range: -181573795840.0 -7618001408.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "tform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),                           # should map [0,255]→[0.0,1.0]\n",
    "    transforms.Lambda(lambda x: x / 255.0),          # <-- force it if ToTensor() is misbehaving\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485,0.456,0.406],\n",
    "        std=[0.229,0.224,0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "\n",
    "# instantiate datasets\n",
    "train_ds = DocumentDataset(\"C:\\\\Users\\\\praag\\\\Desktop\\\\scrapiq\\\\doc scanner\\\\financial classification.v1i.multiclass\\\\train\", '_classes.csv', transform=tform)\n",
    "val_ds   = DocumentDataset(\"C:\\\\Users\\\\praag\\\\Desktop\\\\scrapiq\\\\doc scanner\\\\financial classification.v1i.multiclass\\\\valid\", '_classes.csv', transform=tform)\n",
    "test_ds  = DocumentDataset(\"C:\\\\Users\\\\praag\\\\Desktop\\\\scrapiq\\\\doc scanner\\\\financial classification.v1i.multiclass\\\\test\", '_classes.csv', transform=tform)\n",
    "\n",
    "from collections import Counter\n",
    "train_counts = Counter(label for _, label in train_ds.samples)\n",
    "print(\"✅ Train class counts:\", train_counts)\n",
    "\n",
    "# grab one raw PIL image and one transformed tensor\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# 1) load a raw PIL image\n",
    "raw_path, _ = train_ds.samples[0]\n",
    "raw_img = Image.open(raw_path).convert('RGB')\n",
    "arr = np.array(raw_img)\n",
    "print(\"RAW image pixel range:\", arr.min(), arr.max())\n",
    "\n",
    "# 2) run your transform pipeline manually\n",
    "t = train_ds.transform\n",
    "tensor_img = t(raw_img)\n",
    "print(\"TRANSFORMED tensor shape:\", tensor_img.shape)\n",
    "print(\"TRANSFORMED pixel range:\", tensor_img.min().item(), tensor_img.max().item())\n",
    "print(\"Transform pipeline:\", train_ds.transform)\n",
    "raw_img = Image.open(train_ds.samples[0][0]).convert('RGB')\n",
    "tensor_img = tform(raw_img)\n",
    "print(\"Fixed TRANSFORMED range:\", tensor_img.min().item(), tensor_img.max().item())\n",
    "\n",
    "\n",
    "# loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=32)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5feb813-ecb6-4c22-9748-6f192940029c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch imgs shape: torch.Size([4, 3, 224, 224])\n",
      "Pixel range: -181600174080.0 -278578048.0\n",
      "Labels: tensor([2, 0, 1, 0])\n",
      "Logits range: -5645.06201171875 3628.412109375\n"
     ]
    }
   ],
   "source": [
    "# after defining train_loader, val_loader…\n",
    "import torch\n",
    "from torchvision import models\n",
    "# count classes automatically\n",
    "num_classes = len(train_ds.class_names)\n",
    "device = torch.device(\"cpu\")\n",
    "#torch.set_num_threads(4)\n",
    "\n",
    "# model setup…\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "# Take a small subset of 16 images from training set\n",
    "tiny_subset = Subset(train_ds, list(range(16)))\n",
    "tiny_loader = DataLoader(tiny_subset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Freeze all layers except the final fc\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Re-define optimizer (only for fc)\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3)\n",
    "# Grab one batch\n",
    "imgs, labels = next(iter(tiny_loader))\n",
    "print(\"Batch imgs shape:\", imgs.shape)                 # should be [4,3,224,224]\n",
    "print(\"Pixel range:\", imgs.min().item(), imgs.max().item())\n",
    "print(\"Labels:\", labels)\n",
    "\n",
    "# Forward pass to get raw logits\n",
    "with torch.no_grad():\n",
    "    logits = model(imgs.to(device))\n",
    "print(\"Logits range:\", logits.min().item(), logits.max().item())\n",
    "\n",
    "# print(\"\\n🔁 Overfitting on 16 samples...\\n\")\n",
    "# for epoch in range(20):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     correct = 0\n",
    "#     for imgs, labels in tiny_loader:\n",
    "#         imgs, labels = imgs.to(device), labels.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(imgs)\n",
    "#         loss = loss_fn(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "#         preds = outputs.argmax(dim=1)\n",
    "#         correct += (preds == labels).sum().item()\n",
    "#     acc = 100 * correct / len(tiny_subset)\n",
    "#     print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f}, Accuracy = {acc:.2f}%\")\n",
    "\n",
    "# training + validation loop…\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dff2b075-49ba-41aa-b3be-e2b0badc2f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 — Training Loss: 204.8275\n",
      "Epoch 1/3 — Validation Accuracy: 16.98%\n",
      "\n",
      "Epoch 2/3 — Training Loss: 213.9091\n",
      "Epoch 2/3 — Validation Accuracy: 15.09%\n",
      "\n",
      "Epoch 3/3 — Training Loss: 158.2696\n",
      "Epoch 3/3 — Validation Accuracy: 16.98%\n",
      "\n",
      "✅ Training complete. Model weights saved to model_classify.pt\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the loss function and optimizer\n",
    "loss_fn   = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 2. Training + Validation Loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    # ——— Training Phase ———\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(imgs)\n",
    "        loss    = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass + optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} — Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # ——— Validation Phase ———\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total   = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            preds   = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total   += labels.size(0)\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} — Validation Accuracy: {val_acc:.2f}%\\n\")\n",
    "\n",
    "# 3. Save the trained model weights\n",
    "torch.save(model.state_dict(), \"model_classify.pt\")\n",
    "print(\"✅ Training complete. Model weights saved to model_classify.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6f466f1-5eb4-433c-b65d-d2aa0c60ff84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train class counts: Counter({3: 80, 0: 77, 1: 73, 2: 72})\n",
      "RAW image pixel range: 0 255\n",
      "TRANSFORMED tensor shape: torch.Size([3, 224, 224])\n",
      "TRANSFORMED pixel range: -2.1135387420654297 -1.7870151996612549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\praag\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\praag\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transforms and datasets are now correctly configured. Ready to train!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# 1) Define the corrected transform pipeline\n",
    "tform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),                           # 0–255 → 0.0–1.0\n",
    "    transforms.Lambda(lambda x: x / 255.0),          # force scaling if needed\n",
    "    transforms.Normalize(                           # now normalize to ImageNet stats\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "# 2) (Re)instantiate your datasets with the new pipeline\n",
    "class DocumentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, labels_csv, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        df = __import__('pandas').read_csv(os.path.join(img_dir, labels_csv))\n",
    "        df.columns = df.columns.str.strip()           # strip any spaces\n",
    "        self.class_names = ['cheque', 'payslip', 'receipt', 'tax']\n",
    "        self.class_to_idx = {c:i for i,c in enumerate(self.class_names)}\n",
    "\n",
    "        self.samples = []\n",
    "        for _, row in df.iterrows():\n",
    "            fname = row['filename']\n",
    "            full_path = os.path.join(img_dir, fname)\n",
    "            if not os.path.isfile(full_path):\n",
    "                continue\n",
    "            # find the one-hot label\n",
    "            for cls in self.class_names:\n",
    "                if row[cls] == 1:\n",
    "                    self.samples.append((full_path, self.class_to_idx[cls]))\n",
    "                    break\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Paths to your splits\n",
    "base = r\"C:\\Users\\praag\\Desktop\\scrapiq\\doc scanner\\financial classification.v1i.multiclass\"\n",
    "train_ds = DocumentDataset(os.path.join(base, \"train\"), '_classes.csv', transform=tform)\n",
    "val_ds   = DocumentDataset(os.path.join(base, \"valid\"), '_classes.csv', transform=tform)\n",
    "test_ds  = DocumentDataset(os.path.join(base, \"test\"),  '_classes.csv', transform=tform)\n",
    "\n",
    "# 3) Quick sanity‑checks\n",
    "print(\"✅ Train class counts:\", Counter(label for _, label in train_ds.samples))\n",
    "\n",
    "raw_path, _ = train_ds.samples[0]\n",
    "raw_img = Image.open(raw_path).convert('RGB')\n",
    "arr = np.array(raw_img)\n",
    "print(\"RAW image pixel range:\", arr.min(), arr.max())\n",
    "\n",
    "tensor_img = train_ds.transform(raw_img)\n",
    "print(\"TRANSFORMED tensor shape:\", tensor_img.shape)\n",
    "print(\"TRANSFORMED pixel range:\", tensor_img.min().item(), tensor_img.max().item())\n",
    "\n",
    "# 4) Create your DataLoaders\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=32)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=32)\n",
    "\n",
    "# 5) (Optional) Model setup for the overfit test\n",
    "device = torch.device(\"cpu\")\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, len(train_ds.class_names))\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"\\nTransforms and datasets are now correctly configured. Ready to train!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad1e8523-0027-4263-9ed6-e9047cfbdcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Retesting overfit on 16 training samples...\n",
      "\n",
      "Epoch  1: Loss = 13.5608, Accuracy = 43.75%\n",
      "Epoch  2: Loss = 5.6268, Accuracy = 68.75%\n",
      "Epoch  3: Loss = 2.8739, Accuracy = 75.00%\n",
      "Epoch  4: Loss = 3.8166, Accuracy = 68.75%\n",
      "Epoch  5: Loss = 1.1304, Accuracy = 87.50%\n",
      "Epoch  6: Loss = 1.4768, Accuracy = 81.25%\n",
      "Epoch  7: Loss = 1.6506, Accuracy = 75.00%\n",
      "Epoch  8: Loss = 1.0810, Accuracy = 93.75%\n",
      "Epoch  9: Loss = 0.7262, Accuracy = 93.75%\n",
      "Epoch 10: Loss = 0.4355, Accuracy = 100.00%\n",
      "Epoch 11: Loss = 1.7155, Accuracy = 75.00%\n",
      "Epoch 12: Loss = 0.5343, Accuracy = 100.00%\n",
      "Epoch 13: Loss = 1.1132, Accuracy = 93.75%\n",
      "Epoch 14: Loss = 1.8247, Accuracy = 81.25%\n",
      "Epoch 15: Loss = 0.6081, Accuracy = 100.00%\n",
      "Epoch 16: Loss = 0.6020, Accuracy = 93.75%\n",
      "Epoch 17: Loss = 0.4681, Accuracy = 93.75%\n",
      "Epoch 18: Loss = 1.6566, Accuracy = 87.50%\n",
      "Epoch 19: Loss = 0.8456, Accuracy = 93.75%\n",
      "Epoch 20: Loss = 1.8844, Accuracy = 75.00%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "# 1. Create a tiny subset of 16 samples from your training data\n",
    "# UNFREEZE the whole model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model.train()  # full model in train mode\n",
    "\n",
    "# Recreate optimizer (now includes all model params)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # lower LR for stability\n",
    "loss_fn   = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Overfit on 16 samples again\n",
    "tiny_subset = Subset(train_ds, list(range(16)))\n",
    "tiny_loader = DataLoader(tiny_subset, batch_size=4, shuffle=True)\n",
    "\n",
    "print(\"\\n🔁 Retesting overfit on 16 training samples...\\n\")\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for imgs, labels in tiny_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc = 100 * correct / len(tiny_subset)\n",
    "    print(f\"Epoch {epoch+1:2d}: Loss = {total_loss:.4f}, Accuracy = {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be4add3e-09f3-4c12-8276-d0a85d40b8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0d44abe-df5c-4cbd-8d30-bc5a2fec9b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fe3106a-41ef-433e-8eca-dc27ac91617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x / 255.0),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x / 255.0),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c028324-6ed8-43bb-9c73-7e6952acdc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 — Train Loss: 0.8681 — Train Acc: 79.47% — Val Acc: 26.42%\n",
      "Epoch 2/15 — Train Loss: 0.3125 — Train Acc: 88.74% — Val Acc: 16.98%\n",
      "Epoch 3/15 — Train Loss: 0.1790 — Train Acc: 93.71% — Val Acc: 32.08%\n",
      "Epoch 4/15 — Train Loss: 0.1127 — Train Acc: 96.36% — Val Acc: 16.98%\n",
      "Epoch 5/15 — Train Loss: 0.0816 — Train Acc: 96.36% — Val Acc: 24.53%\n",
      "Epoch 6/15 — Train Loss: 0.0566 — Train Acc: 97.68% — Val Acc: 83.02%\n",
      "Epoch 7/15 — Train Loss: 0.0587 — Train Acc: 98.34% — Val Acc: 58.49%\n",
      "Epoch 8/15 — Train Loss: 0.0466 — Train Acc: 98.34% — Val Acc: 52.83%\n",
      "Epoch 9/15 — Train Loss: 0.0203 — Train Acc: 99.01% — Val Acc: 49.06%\n",
      "Epoch 10/15 — Train Loss: 0.0114 — Train Acc: 99.67% — Val Acc: 86.79%\n",
      "Epoch 11/15 — Train Loss: 0.0145 — Train Acc: 99.67% — Val Acc: 64.15%\n",
      "Epoch 12/15 — Train Loss: 0.0087 — Train Acc: 99.67% — Val Acc: 67.92%\n",
      "Epoch 13/15 — Train Loss: 0.0039 — Train Acc: 100.00% — Val Acc: 90.57%\n",
      "Epoch 14/15 — Train Loss: 0.0010 — Train Acc: 100.00% — Val Acc: 94.34%\n",
      "Epoch 15/15 — Train Loss: 0.0016 — Train Acc: 100.00% — Val Acc: 94.34%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_acc = 100 * correct / total\n",
    "    train_loss = running_loss / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} — Train Loss: {train_loss:.4f} — Train Acc: {train_acc:.2f}% — Val Acc: {val_acc:.2f}%\")\n",
    "    scheduler.step()\n",
    "torch.save(model.state_dict(), \"best_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e80b73a7-e341-4b9a-a534-f3fa7b538a2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3948470738.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[17], line 7\u001b[1;36m\u001b[0m\n\u001b[1;33m    train: Dataset({\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ds4sd/DocLayNet\")\n",
    "\n",
    "dataset\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['image_id', 'image', 'width', 'height', 'doc_category', 'collection', 'doc_name', 'page_no', 'objects'],\n",
    "        num_rows: 69375\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['image_id', 'image', 'width', 'height', 'doc_category', 'collection', 'doc_name', 'page_no', 'objects'],\n",
    "        num_rows: 6489\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['image_id', 'image', 'width', 'height', 'doc_category', 'collection', 'doc_name', 'page_no', 'objects'],\n",
    "        num_rows: 4999\n",
    "    })\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117e8857-edf4-46d5-89d5-a7c03c75248e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
